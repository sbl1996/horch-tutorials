{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using SSD in SVHN dataset\n",
    "\n",
    "In this tutorial, you will learn to:\n",
    "- Use object detection on the Street View House Numbers (SVHN) dataset.\n",
    "- Getting started in SSD using `horch`.\n",
    "\n",
    "Run the following cell to load the packages and dependencies that are going to be useful for your journey!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from horch import cuda\n",
    "from horch.datasets import train_test_split, CocoDetection, Fullset\n",
    "\n",
    "from horch.train import Trainer, Save\n",
    "from horch.train.metrics import TrainLoss, CocoAveragePrecision, COCOEval\n",
    "from horch.train.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from horch.transforms import Compose, ToTensor\n",
    "from horch.transforms.detection import Resize, ToPercentCoords, SSDTransform\n",
    "from horch.transforms.detection.functional import to_absolute_coords\n",
    "\n",
    "from horch.detection import generate_mlvl_anchors, misc_target_collate, draw_bboxes, find_priors_coco\n",
    "from horch.detection.one import MatchAnchors, AnchorBasedInference, MultiBoxLoss\n",
    "\n",
    "from horch.models.utils import summary\n",
    "from horch.models.detection.backbone import ShuffleNetV2\n",
    "from horch.models.detection.enhance import FPN\n",
    "from horch.models.detection.head import SSDHead\n",
    "from horch.models.detection import OneStageDetector\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dataset\n",
    "\n",
    "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images. \n",
    "\n",
    "Dataset Overview:\n",
    "- 10 classes, 1 for each digit. Digit '1' has label 1, '9' has label 9 and '0' has label 10.\n",
    "- 33402 images and 73257 digits for training, 13068 images and 26032 digits for testing\n",
    "\n",
    "Let's load the train set from the prepared COCO format data and check some samples from it with `draw_bboxes` from `horch.detection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./SVHN/train\"\n",
    "ann_file = \"./SVHN/annotations/train.json\"\n",
    "ds = CocoDetection(image_dir, ann_file)\n",
    "print(\"The dataset has totally %d samples.\" % len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because digit '1' has label 1, '9' has label 9, '0' has label 10 and no digit has label 0, we set the argument `categories` to ' 1234567890'. Notice that the first char of `categories` is a blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "img, anns = ds[i]\n",
    "print(\"The size of image of sample #%d is %s\" % (i, img.size))\n",
    "print(\"Annotations: %s\" % anns)\n",
    "fig, ax = draw_bboxes(img, anns, categories=' 1234567890')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "img, anns = ds[i]\n",
    "print(\"The size of image of sample #%d is %s\" % (i, img.size))\n",
    "print(\"Annotations: %s\" % anns)\n",
    "fig, ax = draw_bboxes(img, anns, categories=' 1234567890')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "img, anns = ds[i]\n",
    "print(\"The size of image of sample #%d is %s\" % (i, img.size))\n",
    "print(\"Annotations: %s\" % anns)\n",
    "fig, ax = draw_bboxes(img, anns, categories=' 1234567890')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2579\n",
    "img, anns = ds[i]\n",
    "print(\"The size of image of sample #%d is %s\" % (i, img.size))\n",
    "print(\"Annotations: %s\" % anns)\n",
    "fig, ax = draw_bboxes(img, anns, categories=' 1234567890')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that images from the dataset are of very different sizes (741x350 vs 354x173 vs 199x83 vs 52x23), but with similar aspect ratios (nearly 2:1). And the bounding boxes of digits are also of different sizes, similar aspect ratios (tall and thin) and very close to each other.\n",
    "\n",
    "After inspectation, we decide to resize every image to a propriate size 192x96 and choose feature levels 3, 4, 5 (of stride 8, 16, 32) to learn to detect. Because the bounding boxes are of similar aspect ratios, we assign only 3 anchors (priors) for every feature level (totally 9). We can define them by hand, but it's more precise and convenient to find propriate anchors using `find_priors_coco` in `horch.detection`.\n",
    "`find_priors_coco` can find propriate anchors using k-means (described in YOLOv2). The outputs are percent, we scale them back to absolute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 192\n",
    "height = 96\n",
    "levels = (3, 4, 5)\n",
    "priors_per_level = 3\n",
    "num_classes = 11 # 10 + *1*(background)\n",
    "strides = [2 ** l for l in levels]\n",
    "num_levels = len(levels)\n",
    "\n",
    "priors = find_priors_coco(ds, k=num_levels * priors_per_level, verbose=False)\n",
    "print(\"Generated priors (percent):\")\n",
    "print(priors)\n",
    "anchor_sizes = priors.view(num_levels, priors_per_level, 2) * torch.tensor([width, height], dtype=torch.float32)\n",
    "print(\"Anchors (3 levels, 3 per level, absolute):\")\n",
    "print(anchor_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds very good anchors. The mean of max IoU between anchors and bounding boxes are higher than 0.8. Finally, we use them to generate uniformlly distributed anchors for every feature level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = generate_mlvl_anchors((width, height), strides, anchor_sizes)\n",
    "for a in anchors:\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define transforms for train and test. The `train_transform` resize a sample to the expected size, convert coordinates of bounding boxes to percent, convert the image to tensor and finally match each ground truth box to the anchor with the best IoU and match anchors to any ground truth with IoU higher than a threshold (0.5).\n",
    "For simplicity, we don't use data augmentation and use only a subset to train our model and evalute on the same subset to check how many AP our model can achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    Resize((width, height)),\n",
    "    ToPercentCoords(),\n",
    "    ToTensor(),\n",
    "    MatchAnchors(anchors, pos_thresh=0.5),\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize((width, height)),\n",
    "    ToPercentCoords(),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "rest, ds_small = train_test_split(ds, test_ratio=0.0005)\n",
    "print(\"The small subset has %d samples.\" % len(ds_small))\n",
    "\n",
    "ds_train = Fullset(ds_small, train_transform)\n",
    "ds_val = Fullset(ds_small, test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Model\n",
    "\n",
    "Now, it's the time to define our SSD model, which is very simple using `horch`. Unlike the original SSD, we use a light network ShuffleNetV2 as our backbone rather than VGG. ShuffleNetV2 has 60x fewer parameters (2.3M vs 138M) and similar classification perforamce (69.4% vs 70.5%) compared to VGG16.\n",
    "\n",
    "Let's see how it works.\n",
    "<img src=\"model.png\">\n",
    "<img src=\"get_loc_cls_preds.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load ShuffleNetV2 pretrained on ImageNet from `horch.models.detection.backbone`. `horch.models.detection.backbone` has many different kinds of backbones for different usage scenarios, e.g ResNet, Darknet, ShuffleNetV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ShuffleNetV2(feature_levels=levels, pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show summary of it with `summary` in `horch.models.utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(backbone, (3, height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output channels of backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = backbone(torch.randn(1,3,height,width))\n",
    "for c in cs:\n",
    "    print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All backbones from `horch` have the attribute `out_channels` that returns the output channels of the given feature levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backbone.out_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the head of SSD and check the outputs. Notice that heads in `horch` accept variable number of arguments, so we use list deconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = SSDHead(priors_per_level, num_classes, backbone.out_channels)\n",
    "summary(head, [(116, 12, 24), (232, 6, 12), (1024, 3, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_p, cls_p = head(*cs)\n",
    "print(loc_p.shape)\n",
    "print(cls_p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define inference using `AnchorBasedInference` in `horch.detection.one`. It is suitable for neary all one-stage detection models (SSD, DSSD, FPN, RetinaNet) and can be simply extended to other models (Faster R-CNN, RefineDet).\n",
    "\n",
    "`AnchorBasedInference` inference detections from `loc_p` and `cls_p` based on `anchors`, then filter detections whose confidence is lower than 0.01 and remove highly overlapped detections using non-max suppresion with IoU threshold 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = AnchorBasedInference(cuda(anchors), conf_threshold=0.1, iou_threshold=0.45, nms='nms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it!. Because we have not trained our model, the detection results are random. Notice that the results are also in COCO format, but with percent coordinates.\n",
    "\n",
    "Notice: `inference` use GPU as default, but our model is in CPU now. We must explicitly transform the outputs of the model to GPU and then give them to `inference`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(cuda(loc_p), cuda(cls_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compose them to SSD, which is rather simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "    def __init__(self, backbone, head, inference):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "        self._inference = inference\n",
    "\n",
    "    def forward(self, x):\n",
    "        cs = self.backbone(x)\n",
    "        loc_p, cls_p = self.head(*cs)\n",
    "        return loc_p, cls_p\n",
    "\n",
    "    def inference(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            loc_p, cls_p = self.forward(x)\n",
    "        dets = self._inference(loc_p, cls_p)\n",
    "        self.train()\n",
    "        return dets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SSD(backbone, head, inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training\n",
    "\n",
    "Now, let's see how to train our model.\n",
    "\n",
    "We use `MultiBoxLoss` in `horch.detection.one` as our loss function. Like `AnchorBasedInference`, it is also suitable for neary all one-stage detection models (SSD, DSSD, FPN, RetinaNet) and can be simply extended to other models (Faster R-CNN, RefineDet).\n",
    "\n",
    "<img src=\"train.png\">\n",
    "\n",
    "`Adam` is the default optimizer in most cases which converges quickly and gives acceptable performance. `SGD` with carefully selected hyperparameters gives better performance but needs more time to tune and train.\n",
    "\n",
    "`CosineAnnealingWarmRestarts` decays the learning rate with a cosine annealing.\n",
    "\n",
    "<img src=\"cosinelr.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MultiBoxLoss(p=1) # p represents the probability to print loss\n",
    "optimizer = Adam(filter(lambda x: x.requires_grad,\n",
    "                        net.parameters()), lr=0.001, weight_decay=1e-5)\n",
    "lr_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'loss' as the train-time metric and 'AP' as the test-time metric. Finally, we compose all to a `Trainer` and define \"./checkpoints\" as the path to save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'loss': TrainLoss(),\n",
    "}\n",
    "\n",
    "test_metrics = {\n",
    "    \"AP\": COCOEval(ds_small.dataset.to_coco(ds_small.indices))\n",
    "}\n",
    "\n",
    "trainer = Trainer(net, criterion, optimizer, lr_scheduler,\n",
    "                  metrics=metrics, evaluate_metrics=test_metrics,\n",
    "                  save_path=\"./checkpoints\", name=\"SSD-SVHN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `train_loader` for training and `val_loader` for eval. We use small batch size because our dataset is only a subset of the original dataset. `misc_target_collate` in `val_loader` is necessary for object detection model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    ds_train, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(\n",
    "    ds_val, batch_size=64, collate_fn=misc_target_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train 100 epochs, evaluate every 10 epochs and save model with highmest AP after 80 epochs. Open `localhost:6006` and you can see the metrics online in `Tensorboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(train_loader, 100, val_loader=(val_loader, 10), save=Save.ByMetric(\"val_AP\", patience=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, our model is able to overfit the small subset and achieve AP near 1.0. However, even after training for 100 epochs, our model has not got a good result.\n",
    "\n",
    "The reason is that our model uses Batch Normalization by default, which works very poor when batch size is small.\n",
    "Try to use Group Normalization in backbone and head, then train a new model. After that, the model will converge very quickly and achieve AP near 1.0.\n",
    "\n",
    "Hint: set the argument `norm_layer` to `gn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying Group Normalization and training a very good model on the small subset, let's see how it performs in inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, anns = ds_small[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bboxes(img, anns, categories=\" 1234567890\")\n",
    "print(\"The ground truth is:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_transform(img, anns)[0]\n",
    "x = cuda(x[None])\n",
    "\n",
    "dets = net.inference(x)[0]\n",
    "dets = to_absolute_coords(dets, img.size)\n",
    "\n",
    "draw_bboxes(img, dets, categories=\" 1234567890\")\n",
    "print(\"Our detections are:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you see some obviously wrong detections, set `conf_threshold` of `inference` to a higher value (e.g. 0.3).\n",
    "- If you feel that non-max suppresion didn't work well, set `iou_threshold` of `inference` to a lower value (e.g. 0.25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net._inference.iou_threshold = 0.25\n",
    "net._inference.conf_threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_transform(img, anns)[0]\n",
    "x = cuda(x[None])\n",
    "\n",
    "dets = net.inference(x)[0]\n",
    "dets = to_absolute_coords(dets, img.size)\n",
    "\n",
    "draw_bboxes(img, dets, categories=\" 1234567890\")\n",
    "print(\"Our detections are:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **B** Train a model on the full SVHN dataset and get AP higher than 0.375.\n",
    "- **A** Add FPN (`horch.models.detection.enhance.FPN`) to the model. (Hint: between backbone and head).\n",
    "- **A** Try to replace the backbone with MobileNetV2. It is more powerful than ShuffleNetV2 and as fast as it.\n",
    "- **S** Train a model on the full SVHN dataset and get AP around 0.45 with less than 4M parameters. You may need `horch.models.detection.RefineDet` or `horch.models.detection.FCOS`.\n",
    "- **S** Train a model on your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "OMdut",
   "launcher_item_id": "bbBOL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
